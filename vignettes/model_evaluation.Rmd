---
title: "Model evaluation"
author: "Koen Hufkens"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model evaluation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidymodels)
library(xgboost)
library(ranger)
library(caret)
library(reactable)
source(here::here("R/calc_VI.R"))
source(here::here("R/index_flue.R"))
set.seed(0)

# read in training data
ml_df <- readRDS(
  here::here("data/machine_learning_training_data.rds")
  ) |>
  na.omit()

# create a data split across
# across both droughted and non-droughted days
ml_df_split <- ml_df |>
  rsample::initial_split(
    strata = is_flue_drought,
    prop = 0.8
  )

# select training and testing
# data based on this split
train <- rsample::training(ml_df_split) |>
  select(-is_flue_drought)
test <- rsample::testing(ml_df_split) |>
  select(-is_flue_drought)

```

## Introduction

"fLUE identifies substantial drought impacts that are not captured when relying solely on VPD and greenness changes and, when seasonally recurring, are missed by traditional, anomaly-based drought indices. Counter to common assumptions, fLUE reductions are largest in drought-deciduous vegetation, including grasslands. Our results highlight the necessity to account for soil moisture limitation in terrestrial primary productivity data products, especially for drought-related assessments."

We use fLUE predictions used in previous work as a target in a machine learning based methodology to model this response not based on conventional drought based vegetation indices but using an xgboost based machine learning approach. We used a regression analysis to model fLUE using a 10-fold cross validation and leave-site-out approach.

## Methodology

- fLUE ~70 sites / xyz years
- data split 80/20 between fLUE (<1 threshold based values) 
- 10-fold cross validation
- leave-site-out cross validation (summary stats)
- ...

## Results

### Regression model

Scatterplot of the results, observed vs. predicted fLUE.

```{r echo = FALSE, message=FALSE, warning=FALSE}
# read in precompiled model
regression_model <- readRDS(
  here::here("data/regression_model_spatial.rds")
  )

# run the model on our test data
# using predict()
test_results <- predict(
  regression_model,
  test)$.pred

df <- data.frame(
  test,
  flue_predicted = test_results
  )

p <- ggplot(df,aes(
      flue,
      flue_predicted
    )) +
  geom_abline(slope = 1, intercept = 0) +
  geom_point(
    alpha = 0.2
  ) +
  geom_smooth(method = lm) +
  theme_minimal() +
  facet_wrap(~cluster)

print(p)
```

Table of the regression metrics.

```{r echo=FALSE, message=FALSE, warning=FALSE}
df |>
  yardstick::metrics(
    truth = flue,
    estimate = flue_predicted
    ) |>
  rename(
    metric = .metric,
    value = .estimate
  ) |>
  select(
    metric,
    value
  ) |>
  mutate(
    value = round(value, 3)
  ) |>
  reactable::reactable()

```
Comparing the fLUE response from publication (blue) and modelled values (dark blue).

```{r echo = FALSE, message=FALSE, warning=FALSE}

# read in precompiled model
regression_model <- readRDS(
  here::here("data/regression_model_spatial.rds")
  )


ml_df <- readRDS(
  here::here("data/machine_learning_training_data.rds")
)

ml_df <- index_flue(ml_df)

# run the model on our test data
# using predict()
flue_predicted <- predict(
  regression_model,
  ml_df)$.pred

ml_df$flue_predicted <- flue_predicted

vi <- calc_VI(ml_df, indices = here::here("data/spectral-indices-table.csv"))
ml_df <- bind_cols(ml_df, vi)

# VI to select
VI <- "EVI"

# summary stats across
# clusters and time steps
df2 <- ml_df |>
  filter(
    (n > -20 & n < 100),
    cluster %in% c("cDD", "cGR")
  ) |>
  select(
    cluster,
    n,
    flue,
    flue_predicted,
    !!VI
  ) 

df3 <- df2 |>
  group_by(cluster, n) |>
  summarize(
    # reference
    flue_median = median(flue),
    flue_qt_25 = quantile(flue,0.25),
    flue_qt_75 = quantile(flue,0.75),
    flue_qt_10 = quantile(flue,0.10),
    flue_qt_90 = quantile(flue,0.90),
    
    # estimate
    flue_pred_median = median(flue_predicted, na.rm = TRUE),
    flue_pred_qt_25 = quantile(flue_predicted,0.25, na.rm = TRUE),
    flue_pred_qt_75 = quantile(flue_predicted,0.75, na.rm = TRUE),
    flue_pred_qt_10 = quantile(flue_predicted,0.10, na.rm = TRUE),
    flue_pred_qt_90 = quantile(flue_predicted,0.90, na.rm = TRUE),

    # VI
    VI_median = median(!!sym(VI)),
    VI_qt_25 = quantile(!!sym(VI), 0.25),
    VI_qt_75 = quantile(!!sym(VI), 0.75),
    VI_qt_10 = quantile(!!sym(VI), 0.10),
    VI_qt_90 = quantile(!!sym(VI), 0.90)
  ) |>
  ungroup()


#---- figure ----

p <- ggplot(df3) +
  geom_ribbon(
    aes(
    x = n,
    ymin = flue_qt_25,
    ymax = flue_qt_75
    ),
    fill = "lightblue",
    alpha = 0.2
  ) +
  geom_ribbon(
    aes(
      x = n,
      ymin = flue_qt_10,
      ymax = flue_qt_90
    ),
    fill = "lightblue",
    alpha = 0.2
  ) +
  geom_line(
    aes(
      n,
      flue_median
    ),
    colour = "blue"
  ) +
    geom_line(
    aes(
      n,
      flue_pred_median
    ),
    colour = "darkblue"
  ) +
  geom_ribbon(
    aes(
      x = n,
      ymin = VI_qt_25,
      ymax = VI_qt_75
    ),
    fill = "lightgreen",
    alpha = 0.2
  ) +
  geom_ribbon(
    aes(
      x = n,
      ymin = VI_qt_10,
      ymax = VI_qt_90
    ),
    fill = "lightgreen",
    alpha = 0.2
  ) +
  geom_line(
    aes(
      n,
      VI_median
    ),
    colour = "darkgreen"
  ) +
  theme_minimal() +
  facet_wrap(cluster~.)

print(p)
```

#### Leave-site-out summary stats

Summary statistics for the leave-site-out cross validation.

```{r echo=FALSE, message=FALSE, warning=FALSE}

ml_df <- readRDS(
  here::here("data/machine_learning_training_data.rds")
) |>
  select(site, cluster) |>
  unique()

results <- readRDS(here::here("data/LSO_results.rds"))
results <- left_join(ml_df, results)

# grab test metrics for left out site
tm <- results |>
  group_by(site, cluster) |>
  do({
    . |> yardstick::metrics(truth = flue, estimate = flue_predicted) |>
      dplyr::select(
        .metric,
        .estimate
      ) |>
      rename(
        metric = .metric,
        value = .estimate
      ) |>
      mutate(
        value = round(value, 3)
      )
  }) |>
  pivot_wider(
    values_from = value,
    names_from = metric
  )

reactable::reactable(tm)
```
```{r echo = FALSE, message=FALSE, warning=FALSE}
tm_long <- tm |>
  select(
    rsq,
    site,
    cluster
  ) |>
  tidyr::pivot_longer(
    cols = "rsq",
    names_to = "rsq",
    values_to = "value"
  )

# plot all validation graphs
p <- ggplot(tm_long) +
  geom_boxplot(
    aes(
      cluster,
      value
    )
  ) +
  theme_bw()

print(p)
```

### Regression model landsat

Scatterplot of the results, observed vs. predicted fLUE.

```{r echo = FALSE, message=FALSE, warning=FALSE}

# read in training data
ml_df <- readRDS(
  here::here("data/machine_learning_training_data_landsat.rds")
  ) |>
  na.omit()

# create a data split across
# across both droughted and non-droughted days
ml_df_split <- ml_df |>
  rsample::initial_split(
    strata = is_flue_drought,
    prop = 0.8
  )

# select training and testing
# data based on this split
train <- rsample::training(ml_df_split) |>
  select(-is_flue_drought)
test <- rsample::testing(ml_df_split) |>
  select(-is_flue_drought)

# read in precompiled model
regression_model <- readRDS(
  here::here("data/regression_model_landsat.rds")
  )

# run the model on our test data
# using predict()
test_results <- predict(
  regression_model,
  test)$.pred

df <- data.frame(
  test,
  flue_predicted = test_results
  )

p <- ggplot(df,aes(
      flue,
      flue_predicted
    )) +
  geom_abline(slope = 1, intercept = 0) +
  geom_point(
    alpha = 0.2
  ) +
  geom_smooth(method = lm) +
  theme_minimal() +
  facet_wrap(~cluster)

print(p)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
df |>
  yardstick::metrics(
    truth = flue,
    estimate = flue_predicted
    ) |>
  rename(
    metric = .metric,
    value = .estimate
  ) |>
  select(
    metric,
    value
  ) |>
  mutate(
    value = round(value, 3)
  ) |>
  reactable::reactable()

```

#### Landsat Leave-site-out summary stats

Summary statistics for the leave-site-out cross validation.

```{r echo=FALSE, message=FALSE, warning=FALSE}

ml_df <- readRDS(
  here::here("data/machine_learning_training_data_landsat.rds")
) |>
  select(site, cluster) |>
  unique()

results <- readRDS(here::here("data/LSO_results_landsat.rds"))
results <- left_join(ml_df, results)

# grab test metrics for left out site
tm <- results |>
  group_by(site, cluster) |>
  do({
    . |> yardstick::metrics(truth = flue, estimate = flue_predicted) |>
      dplyr::select(
        .metric,
        .estimate
      ) |>
      rename(
        metric = .metric,
        value = .estimate
      ) |>
      mutate(
        value = round(value, 3)
      )
  }) |>
  pivot_wider(
    values_from = value,
    names_from = metric
  )

reactable::reactable(tm)
```

```{r echo = FALSE, message=FALSE, warning=FALSE}
tm_long <- tm |>
  select(
    rsq,
    site,
    cluster
  ) |>
  tidyr::pivot_longer(
    cols = "rsq",
    names_to = "rsq",
    values_to = "value"
  )

# plot all validation graphs
p <- ggplot(tm_long) +
  geom_boxplot(
    aes(
      cluster,
      value
    )
  ) +
  theme_bw()

print(p)
```
